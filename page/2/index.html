<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Wens'Blog - 永远相信美好的事情即将发生。</title><meta name="author" content="Wens"><meta name="copyright" content="Wens"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Wens&#39;Blog">
<meta property="og:url" content="http://reveryday.github.io/page/2/index.html">
<meta property="og:site_name" content="Wens&#39;Blog">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://reveryday.github.io/images/wens.png">
<meta property="article:author" content="Wens">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://reveryday.github.io/images/wens.png"><link rel="shortcut icon" href="/images/logo.png"><link rel="canonical" href="http://reveryday.github.io/page/2/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":false,"highlightLang":false,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Wens\'Blog',
  isHighlightShrink: undefined,
  isToc: false,
  pageType: 'home'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/images/wens.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user-circle"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Friends</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user-circle"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Friends</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="site-info"><h1 id="site-title">Wens'Blog</h1><div id="site-subtitle"><span id="subtitle"></span></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout hide-aside" id="content-inner"><div class="recent-posts nc" id="recent-posts"><div class="recent-post-items"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/03/21/RL-01-Introdunction/" title="RL-01-Introdunction">RL-01-Introdunction</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-21T13:52:09.000Z" title="发表于 2025-03-21 21:52:09">2025-03-21</time></span></div><div class="content">  监督学习：学习给定目标（标签） 无监督学习：学习没有标签的数据中的规律 半监督学习 强化学习  Q-Learning</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/03/21/TrainLog-MyModel/" title="TrainLog-MyModel">TrainLog-MyModel</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-21T06:38:26.000Z" title="发表于 2025-03-21 14:38:26">2025-03-21</time></span></div><div class="content">Load-data$$N&#x3D;B{\cdot}N_{0}^{-\mu d}&#x3D;B{\cdot}N_{0}^{-\frac{\mu}{\rho}d{\cdot}{\rho}}&#x3D;B{\cdot}N_{0}^{-{\mu_{m}}d_{m}}$$ 可转换为： $$log(B)&#x3D;log(\frac{N}{N_{0}})+{\mu_{m}}d_{m}log(e)$$ （1）筛选数据    列名称 最大值 最小值    MAC_Incoherent 0.162525 0.01213746   MAC_Coherent 0.1889905 5.611962e-07   MAC_Photoelectric 5.255423 1.386763e-07   MAC_Pair_production 0.0369793 0.0   eff-buf 1987043000.0 1.077671   可以剔除掉0.1mev含铅屏蔽层与0.15mev含铅屏蔽层的情况，我选择了把累积因子10e+5以上的数据全部删掉。 （2）对数据进行预处理：  取log(y+1)Epoch...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/03/20/DL00-Question/" title="DL0-Question">DL0-Question</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-20T07:58:51.000Z" title="发表于 2025-03-20 15:58:51">2025-03-20</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Deep-Learning-PyTorch/">Deep Learning-PyTorch</a></span></div><div class="content">Q1: 为什么sgd参数更新能使得拟合效果变好？ Q2：为什么ReLu激活函数能支持神经网络的非线性建模能力？  根据通用逼近定理，具有足够多神经元和 ReLU 激活的单隐藏层网络可以逼近任何连续函数，多层网络则更高效。 </div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/03/17/DL11-Transformer/" title="DL11-Transformer">DL11-Transformer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-17T11:27:14.000Z" title="发表于 2025-03-17 19:27:14">2025-03-17</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Deep-Learning-PyTorch/">Deep Learning-PyTorch</a></span></div><div class="content"> Transformer是由编码器和解码器组成的，编码器和解码器是纯基于自注意力的模块叠加而成的，源（输入）序列和目标（输出）序列的嵌入（embedding）表示将加上位置编码（positional encoding），再分别输入到编码器和解码器中。 Scaled Dot-Product Attention Multi-head atttention 对同一query、key、value希望抽取不同的信息（如长距离关系与短距离关系），使用h个独立的注意力池化，合并各个head输出得到最终输出。  基于位置的前馈网络基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP），这就是称前馈网络是基于位置的（positionwise）的原因。 残差连接和层规范化(add&amp;norm)Position-Encoding </div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/03/17/DL10-attention-mechanism/" title="DL10-Attention Mechanism">DL10-Attention Mechanism</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-17T08:24:21.000Z" title="发表于 2025-03-17 16:24:21">2025-03-17</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Deep-Learning-PyTorch/">Deep Learning-PyTorch</a></span></div><div class="content"> 灵长类动物的视觉系统接受了大量的感官输入， 这些感官输入远远超过了大脑能够完全处理的程度。 然而，并非所有刺激的影响都是相等的。 意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，例如猎物和天敌。 只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。  query-value-key在注意力机制的背景下，我们将自主性提示称为查询(Query)。对于任何给定查询，注意力机制通过集中注意力(Attention Pooling)选择感官输入(Sensory Input),这些感官输入被称为值(Value)。每个值都与其对应的非自主提示的一个键(Key)对应，通过集中注意力，为给定的查询(自主性提示)与键(非自主性提示)进行交互，从而引导选择偏向值(感官输入)  注意力机制与全连接层或者汇聚层的区别源于增加的自主提示query，query（自主提示）和key（非自主提示）之间的交互形成了注意力汇聚，注意力汇聚有选择地聚合了value以生成最终的输出。 注意力机制 实现：编码器与解码器都可以采用RNN模型， ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/03/16/DL9-rnn-modern/" title="DL9-rnn-modern">DL9-rnn-modern</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-16T14:03:11.000Z" title="发表于 2025-03-16 22:03:11">2025-03-16</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Deep-Learning-PyTorch/">Deep Learning-PyTorch</a></span></div><div class="content">GRU（门控循环单元）门控循环单元与普通的循环神经网络之间的关键区别在于： 前者支持隐状态的门控，GRU有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态，这些机制是可学习的。    重置门$\mathbf{R}_t \in \mathbb{R}^{n \times h}$和更新门$\mathbf{Z}_t \in \mathbb{R}^{n \times h}$的计算如下所示： $$ \begin{aligned} \mathbf{R}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1} \mathbf{W}_{hr} + \mathbf{b}_r)\\ \mathbf{Z}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xz} + \mathbf{H}_{t-1} \mathbf{W}_{hz} +...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/03/16/DL8-recurrent-neural-networks/" title="DL8-recurrent-neural-networks">DL8-recurrent-neural-networks</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-16T13:24:03.000Z" title="发表于 2025-03-16 21:24:03">2025-03-16</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Deep-Learning-PyTorch/">Deep Learning-PyTorch</a></span></div><div class="content">到目前为止，我们遇到过两种类型的数据：表格数据和图像数据。 对于表格数据，mlp可以比较好的处理，而对于图像数据，设计了专门的CNN架构来为这类特殊的数据结构建模，可以有效处理单个样本的空间信息。然而，1、到目前为止我们默认数据都来自于某种分布， 并且所有样本都是独立同分布的，大多数的数据并非如此。2、另一个问题来自这样一个事实： 我们不仅仅可以接收一个序列作为输入，而是还可能期望继续猜测这个序列的后续。  CNN可以有效处理空间信息，RNN可以处理时间信息，或者样本之间并非独立同分布的数据。  卷积神经网络可以有效地处理空间信息， 那么循环神经网络（RNN）则可以更好地处理序列信息，通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。 自回归模型 $t &gt; \tau$时，用观测序列$x_{t-1}, \ldots, x_{t-\tau}$$x_{\tau}$预测当下，参数的数量不变，这种模型被称为自回归模型，因为它们是对自己执行回归   2. 保留一些对过去观测的总结$h_t$，同时更新预测$\hat{x}_t$和总结$h_t$，基于$\hat{x}_t...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/03/16/DL7-The-key-to-Train-Deep-Networks/" title="DL7-The key to Train Deep Network">DL7-The key to Train Deep Network</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-16T12:48:26.000Z" title="发表于 2025-03-16 20:48:26">2025-03-16</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Deep-Learning-PyTorch/">Deep Learning-PyTorch</a></span></div><div class="content">一些指标 平均绝对误差 (MAE, Mean Absolute Error)  $$MAE &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} |y_i - \hat{y}_i|$$  均方根误差 (RMSE, Root Mean Squared Error)$$RMSE &#x3D; \sqrt{\frac{1}{n} \sum_{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2}$$  均方误差 (MSE, Mean Squared Error)$$MSE &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2$$  平均绝对百分比误差 (MAPE, Mean Absolute Percentage Error)$$MAPE &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right| \times 100$$  决定系数 (R², Coefficient of...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/03/16/DL6-conv-modern/" title="DL6-conv-modern">DL6-conv-modern</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-16T08:58:42.000Z" title="发表于 2025-03-16 16:58:42">2025-03-16</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Deep-Learning-PyTorch/">Deep Learning-PyTorch</a></span></div><div class="content">AlexNet训练神经网络的一些关键技：启发式参数初始化、随机梯度下降的变体、非挤压激活函数和有效的正则化技术  从对最终模型精度的影响来说，更大或更干净的数据集、或是稍微改进的特征提取，比任何学习算法带来的进步要大得多。    activation：ReLU激活函数使训练模型更加容易，当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。 相反，ReLU激活函数在正区间的梯度总是1。 因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。 容量控制与预处理：AlexNet通过暂退法控制全连接层的模型复杂度，而LeNet只使用了权重衰减。为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色，这使得模型更健壮，更大的样本量有效地减少了过拟合。 VGG使用块的想法首先出现在牛津大学的视觉几何组（visual geometry...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/03/03/DL5-conv%20elements/" title="DL5-Conv Elements">DL5-Conv Elements</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-03T13:38:03.000Z" title="发表于 2025-03-03 21:38:03">2025-03-03</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Deep-Learning-PyTorch/">Deep Learning-PyTorch</a></span></div><div class="content">卷积层的输入与输出：输入形状为$n_h\times n_w$，卷积核形状为$k_h\times k_w$，那么输出形状将是$(n_h-k_h+1) \times...</div></div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/#content-inner">3</a><a class="extend next" rel="next" href="/page/3/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2025 By Wens</div><div class="footer_custom_text">天道酬勤</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>window.typedJSFn = {
  init: str => {
    window.typed = new Typed('#subtitle', Object.assign({
      strings: str,
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50,
    }, null))
  },
  run: subtitleType => {
    if (true) {
      if (typeof Typed === 'function') {
        subtitleType()
      } else {
        btf.getScript('https://cdn.jsdelivr.net/npm/typed.js/dist/typed.umd.min.js').then(subtitleType)
      }
    } else {
      subtitleType()
    }
  }
}
btf.addGlobalFn('pjaxSendOnce', () => { typed.destroy() }, 'typedDestroy')
</script><script>function subtitleType () {
  if (true) {
    typedJSFn.init(["永远相信美好的事情即将发生。"])
  } else {
    document.getElementById("subtitle").textContent = "永远相信美好的事情即将发生。"
  }
}
typedJSFn.run(subtitleType)</script></div></div></body></html>